\documentclass[twocolumn]{article}

\usepackage{graphicx}
\usepackage{biblatex}
\usepackage{microtype}
\usepackage{fullpage}

\title{Benchmarks for Energy Measurements on Embedded Platforms}
\author{James Pallister, Simon Hollis, Jeremy Bennett}

\begin{document}
\maketitle
\section*{Abstract}

\textit{ABSTRACT}

\section*{Introduction}

Benchmarking is frequently used to gain an idea of how a system will perform during its general use, when the specific environment cannot be reproduced at design-time. This gives designers feedback on how their system will perform and where its performance is lacking. Typically one benchmark cannot exercise all aspects of a the target, leading to suites of benchmarks of which each tests a combination of areas of the hardware. This separation of benchmarks allows the design to see which parts of the hardware perform the best.


Many sets of benchmarks exists already, such as MiBench\cite{MiBench}, MediaBench\cite{MediaBench}, LINPACK\cite{LINPACK}, Dhrystone\cite{Dhrystone} and more. These are all targeted towards larger desktop-based applications, with significant compute power. Most at least assume a host operating system, which may not be present on an embedded system. Further more, when analysing energy consumption, having to account for the operating system’s effect on the result is non-trivial. These benchmarks - while in theory are portable - have significant difficulties running unmodified on embedded platforms due to memory and storage constraints. The issue with portability also arises when it is necessary to constrast multiple platforms.

The energy consumption of our electronic devices is rapidly becoming a large factor in the design process. An embedded system will typically have severe power restraints placed upon it, if it is to have a long battery life. To recognise whether these constraints have been met, the power consumption of the device MiBench established a well known set of benchmarks must be tested. To build a full picture of a platform’s energy consumption characteristics, a benchmark suite that hits possible combinations of an applications characteristics (such as memory accesses, integer and floating point operations, etc) is needed. This allows the energy consumption of various components of the system to be determined, as well as exposing compiler effects.

Currently a good benchmark suite for measuring energy consumption is not available for embedded platforms. MiBench is the closest but assumes there is a host operating system for the majority of the benchmarks. In particular it requires access to files, whereas on embedded platforms there may not even be a filesystem.

In this paper a set of benchmarks, chosen from other popular benchmark suites is presented, along with justification for their use in benchmarking energy consumption. The selection was designed such that they benchmarks would be cross platform, exercising aspects of the processor and memory system. The benchmarks are intended to be run on the ‘bare metal’ with no host operating system.

TALK ABOUT 4 main points here

Six different platforms are explored:
\begin{itemize}
	\item STM32F0DISCOVERY - ARM Cortex-M0
	\item STM32VLDISCOVERY - ARM Cortex-M3
	\item BeagleBone - ARM Cortex-A8
	\item Adapteva Epiphany
	\item XMOS L1
	\item ChipKIT Uno - PIC32MX (MIPS)
\end{itemize}

\section*{Previous Work}

TALK ABOUT HOW OUR SUITE COMPARES TO OTHERS

MiBench established a well known set of benchmarks with well characterised behaviour. This suite consisted of 37 different benchmarks split across six different categories, chosen to be representative of which applications would be run on both desktop and embedded platforms. Each benchmark is justified, with instruction traces analysed on a model of the StrongARM architecture. This gave a good overview of the proportions of eachtypes of instructions that the benchmarks executed. The drawback of this was that the instruction traces were only gathered for one platform - each benchmark could have a radically different instruction distribution for alternative platforms.

TODO Milepost uses mibench. talk some more
about milepost.

TODO talk about parmibench. other parallel
benchmark suites targeted at HPC.

DSPstone[DSPstone] is a benchmark suite targeted at Digital Signal Processors (DSPs) and was originally designed to evaluated compiler effectiveness at compiling for DSPs. This suite contains a large amount of non-integer tests, with most tests replicated in fixed point and floating point form. BLAH BLAH. As this set is aimed at DSPs rather than general purpose processors no benchmarks were chosen from DSPstone. The lack of an explicit license attached to these applications also makes it difficult to redistribute them.

A set of benchmarks is maintained by the worst case execution time (WCET) initiative[WCET]. These benchmarks are good in that they are very self contained, written completely in C. Each benchmark is less comprehensive than from the MiBench set, but focusses on one particular application that may be specifically what a low end processor will perform.

Several other benchmark suites were evaluated. These were:
\begin{itemize}
	\item MediaBench
	\item OpenBench\cite{OpenBench}
	\item SPEC2000\cite{SPEC2000}
	\item LINPACK
	\item Livermore Fortran Kernels
\end{itemize}

All of these benchmark suites were found to be unsuitable for the aim of characterising energy consumption on embedded platforms due to their reliance on the operating system and features provided by it.

\section*{Platforms}

A range of platforms has been chosen, covering different types of architectures for different purposes. The processors are mainly small architectures which are designed for low power usage. As a consequence some of the platforms are very memory limited, restricting the types of applications that would be run on them.

TALK ABOUT PLATFORMS IN ABSTRACT, FEATURES DESIRED FROM THEM

\section*{Benchmarks}

\begin{table*}
\centering
	\begin{tabular}{l c c c c c c c l}
	Name				 	& Source 	& B & M & I & FP& T 		& License & Category \\
	\hline
	crc32					& MiBench 	& L & L & H & L & Single 	& GPL	& network, telecomm	\\
	primes					& WCET 		& M & L & H & L & Multi 	& None	& automotive	\\
	integer matmult			& WCET	 	& M & M & H & L & Single 	& None	& automotive	\\
	cubic root solver		& MiBench 	& L & L & L & H & Single 	& GPL	& automotive	\\
	2d fir					& WCET 		& M & H & L & H & Single 	& None	& automotive, consumer	\\
	float matmult			& WCET 		& M & M & L & H & Single 	& GPL	& automotive, consumer	\\
	arithmetic compression	&			& M & M & L & M & Single 	& GPL	& consumer, telecomm	\\
	dijkstra				& MiBench 	& H & M & M & L & Multi 	& GPL	& network	\\
	patricia				& MiBench 	& H & H & L & L & Multi 	& GPL	& network	\\
	blowfish				& MiBench 	& L & M & H & L & Multi 	& GPL	& security	\\
	rjindael				& MiBench 	& L & M & H & L & Multi 	& GPL	& security	\\
	triple-des				&		 	& L & M & H & L & Multi 	& GPL	& security	\\
	sha						& MiBench 	& L & L & H & L & Multi 	& GPL	& network, security	\\
	fft						& MiBench 	& M & H & L & H & Multi?	&  GPL	& automotive, telecomm	\\
	fdct					& WCET 		& M & H & H & L & Single 	& None	& consumer	\\
	\end{tabular}
\caption{Benchmarks selected, and the categories they fit in. Legend in Table~\ref{BenchmarkLegend}}
\end{table*}


A set of benchmark to to tests all aspects of the target platforms has been found. Individual tests from several different benchmark suites were considered. In particular MiBench had a large amount of well defined benchmarks, however a large proportion of these were targeted at much higher end platforms than chosen. This lead to a small subset of the MiBench benchmarks being selected. Several benchmarks were sourced from the WCET set. These tested small applications which could conceivably be ran by the platforms discussed earlier.

MiBench divides the embedded processor applications into six categories: automotive, network, consumer, security, telecomms and office. The benchmarks selected broadly fit into these categories, however consumer and office in particular require the higher end embedded processors. This is due to the benchmarks running `off the shelf' programs such as ghostscript and rsynth.

There were four orthoganal aspects to each benchmark that had to be considered:
\begin{itemize}
	\item Floating point vs integer operations
	\item Memory access intensity
	\item Multithreaded-ness
	\item Branding frequency
\end{itemize}

These categories roughly correspond to the types of instructions seen in the instruction traces from MiBench. By using benchmarks that hit different points of these aspects, interesting observations about the enery consumption of the device can be made. 

The final set of benchmarks is shown in Table~\ref{Benchmarks}.

\begin{table}
\centering
	\begin{tabular}{c l}
		Key & Description \\
		\hline
		L	&	Low \\
		M	&	Medium \\
		H	&	High \\
		\hline
		B	&	Branching \\
		M	&	Memory intensity \\
		I	&	Integer pipeline intensity \\
		FP	&	FPU pipeline intensity \\
		T	&	Threaded-ness \\
	\end{tabular}
	\caption{Legend for the benchmark table}
	\label{BenchmarkLegend}
\end{table}

\section*{Benchmark Analysis}

This section talks about each benchmarks, giving a short description of the benchmark and why it is included. A breakdown of the types of instructions in the benchmark is given for each platform.

\subsubsection*{Blowfish}

Blowfish is an encryption algorithm commonly used in cryptography. This benchmark was taken from MiBench but modified to encrypt small blocks of data. Encryption typically involves many integer operations with fewer branches.

\subsubsection*{Rijndael}

Rijndael is the algorithm for the Advanced Encryption Standard. It is commonly used in many security applications


breakdown of types of sinstruciton per bnehcmark per platform

\section*{Evaluation}

With base energy measurements for each instruction, estimate amount of energy each benchmark should take from proportion of instructions. Compare these to actual figures from the benchmarks being run

\section*{Conclusion}

\printbibliography

\end{document}