\documentclass[twocolumn]{article}

\usepackage{graphicx}
\usepackage{biblatex}
\usepackage{microtype}
\usepackage{fullpage}
\usepackage{titlesec}

\title{Identifying Compiler Options to Minimize Energy Consumption for Embedded Platforms}
\author{James Pallister, Simon Hollis, Jeremy Bennett}

% \titleformat{\section}{}{\centering}{0em}{}

\begin{document}
\maketitle
\section*{Abstract}

\textit{ABSTRACT}

\section*{Introduction}

Talk about energy consumption in general

talk about embedded platforms

GCC Talk compiler optimisations affecting performance. Many people think that this has been done, but no extensive data gathered on. Mention iterative compilation paper: only looked at 2 optimisations for 3 limited test cases. Want to know if faster program=less energy.

Talk about how different benchmarks are needed, taken from contempory, chosen to be implementable,
short paragraph, quick justification.

\begin{table*}
\centering
	\begin{tabular}{l c c c c c c c l}
	Name				 	& Source 	& B & M & I & FP& T 		& License & Category \\
	\hline
	crc32					& MiBench 	& L & L & H & L & Single 	& GPL	& network, telecomm	\\
	integer matmult			& WCET	 	& M & M & H & L & Single 	& None	& automotive	\\
	cubic root solver		& MiBench 	& L & L & L & H & Single 	& GPL	& automotive	\\
	2d fir					& WCET 		& M & H & L & H & Single 	& None	& automotive, consumer	\\
	float matmult			& WCET 		& M & M & L & H & Single 	& GPL	& automotive, consumer	\\
	dijkstra				& MiBench 	& H & M & M & L & Multi 	& GPL	& network	\\
	blowfish				& MiBench 	& L & M & H & L & Multi 	& GPL	& security	\\
	rjindael				& MiBench 	& L & M & H & L & Multi 	& GPL	& security	\\
	sha						& MiBench 	& L & L & H & L & Multi 	& GPL	& network, security	\\
	fdct					& WCET 		& M & H & H & L & Single 	& None	& consumer	\\
	\end{tabular}
\caption{Benchmarks selected, and the categories they fit in. Legend in Table~\ref{BenchmarkLegend}}
\end{table*}


There are 5 platforms examined in this paper:
BLAH

This paper covers the design and results of the experiment. First the experimental design is discussed, followed by a

\section*{Previous work}

Mention not much previous work. Lots of work done on performance, and mostly assumed to carry over. Not much done with real hardware

Talk a lot about iterative compilation for energy paper. lack of benchmarks, only looks at loop unrolling and tilesize (todo check). The only tests they did were matrix multiplication, vector by matrix multiplication (and one more). Also only one architecture. The energy consumption might be very dependant on the architecture and way the pipeline is structured, so their results may not be applicable to other platforms.

performance==energy, only tested the overall optimization levels O1-O4 and 4 individual optimizations, simulated with wattch - inherits all the inaccuracies that wattch might have

\section*{Hardware Setup}

A few paragraphs about the hardware setup.
Talk about INA219, error bounds, analysis of error?
Talk about hooking the inductors

No OS

\section*{Experimental Design}

Fractional Factorial design. Mention intel paper

Quick overview of how FFD works. Introduce resolutions, mention that compiler options should possible have many highorder interactions

All FFDs used were generated by R.

\section*{Compiler Options Examined}

talk about the experiments we actually did

For each platform and benchmark combination the following experiments were ran.

\subsection*{First Optimization Level (O1)}

A FFD design of resolution 5 was used to evaluate GCC's 37 flags turned on by \texttt{-O1}. This consisted of 2048 different runs, allowing the main factors to be examined. If third order interactions are considered negligible, two way interactions of the flags can be resolved. The experiment aimed to identify which of the flags in this optimization level has the largest

\subsection*{Second Optimization Level (O2)}

As with thie \texttt{O1} experiment this uses a resolution 5 FFD to examine the 36 factors enabled by \texttt{-O2}. This optimization level enables more advanced

\subsection*{Incremental Tests}

This experiment used to data generated by the \texttt{O1} tests to cumulatively turn on the flags in order of best to worst. When compared to having all the flags on those shows the effect that a small number of optimisations can have on the code.

\subsection*{Single Flag Difference}

This experiment enabled the first optimization level (\texttt{-O1}) then individually added or subtracted possible optimisation flags. The aim of this experiment was to identify if a single flag could have a significant difference on the energy consumption.

\subsection*{Profiling}

Using profile guided feedback the compile can make much better guesses about what the code is going to do. This allows extra optimizations to be enabled and improves to performance of some existing optimizations.

\subsection*{Link Time Optimization}

When the benchmark consists of multiple compilation units, the program is not optimized as a whole. This means many of the optimizations performed could be better if done at link time.

\section*{Results}

Basic results table, matrix - columns are platforms, rows are benchmarks, O0, O1, O2, O3 results in each cell?
Could have a small 4 point line graph in each cell, percentage improvement over baseline (O0)

present interesting graphs

\subsection*{Time and Energy}

mostly true.
some cases when not the case. why is this?
suggest in some cases, without changing performance you can reduce energy

talk when arent the same.
reasons for that

% talk about what the results are briefly
% second and third order interactions

count
by eye add up significant factors, how many times occur across benchmark, platform
generality - specific to benchmarks or

point some option good or bad, cant really predict
exhaustive search

\section*{Modelling}

Talk about how the FFD can be used to predict what flags are good

\section*{Comparison with LLVM}

Just O1, O2, O3


\section*{Evaluation}

Talk about error
 - hardware measurement
 - total amonut of energy used is correct, even if sample rate is low
 - FFD
 - cold start problem
 	only affects beaglebone, as no caches otherwise, but fits in cache anyway


Draw correlations between platforms and benchmarks.

See if there is any correlation between the `execution characteristics' of the benchmark and the flags that affect it.

Can we propose a set of options that on average performs better for the benchmarks?

Why did we get these results


\section*{Conclusion}

\section*{Future Work}

\printbibliography

\end{document}
